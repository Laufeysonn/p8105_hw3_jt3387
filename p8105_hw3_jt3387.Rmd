---
title: "p8105_hw3_jt3387"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Load the dataset:

```{r}
data("instacart")
instacart = instacart %>% 
  as_tibble(instacart)
```
  
Brief discription of the dataset:

- This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.

- The dataset contains following variables: ``r names(instacart)``, and some key variables are: `order_id` represents the order identifier, `product_id` represents the product identifier, `user_id` represents the customer identifier, `aisle_id` represents the aisle identifier, `department_id` represents the department identifier.
  
- In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

- In total, there are 134 aisles, and fresh vegetables and fresh fruits aisles are the most items ordered from.

Make a plot that shows the number of items ordered in each aisle.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. 

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r message = FALSE}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

## Problem 2

Load, tidy, and wrangle the data.

```{r message = FALSE}
accel_df <- 
  read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_prefix = "activity_",
    names_to = "minute",
    values_to = "activity"
  ) %>% 
  mutate(
    minute = as.numeric(minute),
    dow = ifelse(day %in% c('Saturday', 'Sunday'), "weekend", "weekday")
  ) %>% 
  relocate(week, day_id, day, dow)
```

Brief discription of the resulting dataset:

- The data contains `r nrow(accel_df)` rows and `r ncol(accel_df)` columns.

- There are 7 variables in the data set: `week` represents the week number from 1 to 5, `day_id` shows the id of each day ranging from 1 to 35, `day` shows what day of the week it is, `dow` shows whether the day is a weekday or weekend, `minite` represents every minute within a 24-hour day ranging from 1 to 1440, `activity` shows the activity counts for each minute of a 24-hour day starting at midnight, and its min value is ``r min(pull(accel_df, activity))`` and max value is ``r max(pull(accel_df, activity))``.

Analyses of the data focus on the total activity over the day.

```{r message = FALSE}
total_act <- 
  accel_df %>% 
  group_by(week, day) %>% 
  summarize(sum_activity = sum(activity))
```

Create a table showing the totals.

```{r}
total_act %>% 
  pivot_wider(
    names_from = "day",
    values_from = "sum_activity"
  ) %>% 
  relocate(1, 5, 3, 7, 8, 6, 2, 4) %>% 
  knitr::kable(
    caption = "Total activity over a day",
    align = "l"
  )
```

Plot the data to see if there is a trend:

```{r}
total_act %>% 
  mutate(
    day = factor(day, levels = c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'))
  ) %>% 
  ggplot(aes(x = day, y = sum_activity, group = week, color = week)) + 
  geom_point() + 
  geom_line()
```

- From the chart, we can see that the total activity count in five weeks has been fluctuating, and the fluctuations in the forth and fifth weeks are relatively greater. Overall, the total activity number on weekends is relatively lower than on weekdays.

Make a plot to shows the 24-hour activity time courses for each day.

```{r}
accel_df %>% 
  mutate(hour = minute / 60) %>% 
  ggplot(aes(x = hour, y = activity, color = day)) + 
  geom_line(alpha = 0.8) + 
  labs(
    x = "Hours",
    y = "Activity",
    title = "24-hour activity time courses"
  )
```

- From the plot, we can see that the activity counts are much lower from 11:00 pm to 5:00 am in the midnight and early morning, whichcorresponds to the sleep time, and the high activity counts are concentrated from 9:00 am to 12:00 am and 8:00 pm to 10:00 pm. Then a moderate activity counts appears from 4:00 pm to 5:00 pm.

## Problem 3

Load and summary the dataset.

```{r}
data("ny_noaa")
ny_noaa <- ny_noaa %>% 
  mutate(tmin = as.numeric(tmin), 
         tmax = as.numeric(tmax)) %>% 
  as_tibble(ny_noaa)
summary(ny_noaa)
```

Brief description of the dataset:
  
- The data contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns.

- The dataset contains following variables: ``r names(ny_noaa)``, and some key variables are: `id` represents the weather station ID, `data` represents the date of observation, `prcp` represents precipitation(tenths of mm), `snow` represents snowfall(mm), `snwd` represents snow depth(mm), `tmax` represents maximum temperature (tenths of degrees C), `tmin` represents minimun temperature(tenths of degrees C).

- The data was collected by `r length(unique(ny_noaa$id))` weather stations from 01/01/1981 to 12/31/2010. By using `summary` function, we can also see that there are lots of missing values in variables: `prcp`, `snow`, `snwd`, `tmax`, `tmin`, and nearly 5.6% of `prcp`, 14.7% of `snow`, 22.8% of `snwd`, and over 40% of `tmin` and `tmax` values are missing. If the missing is not at random, there will be bias in the data and further statistical analysis result will be affected.

Clean the data.

```{r}
ny_noaa <- ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c('year', 'month', 'day'), sep = "-") %>%
  mutate(
    tmax = tmax/10,
    tmin = tmin/10,
    prcp = prcp/10,
    year = as.numeric(year),
    month = as.numeric(month),
    day = as.numeric(day))
```

- We should divide `tmin`, `tmax`, `prcp` by 10 to keep them in  degrees C and mm. Then as we convert `tmin`, `tmax` into numeric before, we will also convert `year`, `month`, `day` into numeric format. 

Quantitative analysis of snowfall.

```{r}
ny_noaa %>%
  group_by(snow) %>%
  summarise(snowfall_obs = n()) %>%
  arrange(desc(snowfall_obs))
```

- Therefore, the most commonly observed values for snowfall is 0. Because snow only occurs in some of the winter days, most of the time in a year there is no snowfall.

Make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r message = FALSE, warning = FALSE}
ny_noaa %>%
  mutate(month = month.name[month]) %>%
  group_by(id, year, month) %>%
  filter(month %in% c ("January", "July")) %>%
  summarise(mean_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_tmax, color = month)) +
  geom_point() +
  geom_smooth() +
  labs(x = "Year", y = "Average Max Temperature",
       title = "Average max temperature in January and in July from 1981 to 2010") +
  facet_grid(.~month)
```

- From the plot, we can see that the average max temperature in January from 1980 to 2010 was approximately fluctuating around 0 degree C on a ten-year cycle, while in July the average max temperature was approximately fluctuating around 27 degree C slightly. There are many outliers in the data, like the lowest temperature point in January in 1982 and lowest temperature point in July in 1988.

Make a two-panel plot showing: 

(i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option);

(ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r warning = FALSE, message = FALSE}
tmax_tmin <- ny_noaa %>%
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() + 
  labs(x = "Minimum temperature (degrees C)", y = "Maximum temperature (degrees C)", 
       title = "tmax vs tmin for the full dataset") +
  theme(legend.text = element_text(size = 5))

snowfall_dis <- ny_noaa %>%
  filter(snow > 0 & snow < 100) %>%
  mutate(year = as.character(year)) %>%
  ggplot(aes(x = snow, y = year, fill = year)) +
  geom_density_ridges(alpha = .7, scale = .5) +
  labs(x = "Snowfall (mm)",y = "Year", title = "Distribution of snowfall by year") +
  theme(legend.position = "none")

tmax_tmin+snowfall_dis
```

- From the first plot, we can see that there are lots of points on which the min temperature is around 15 degrees C while the max temperature is around 22 degrees C, and min temperature is around 0 degrees C while the max temperature is around 3 degrees C. The distribution of points is concentrated, and there are also some outliers. From the second plot, which shows the distribution of the snowfall between 0 and 100, we can see that the overall distribution from different years didn't change much, and most of the snowfall amounts are concentrated around 10 mm, 23mm, and 50mm.